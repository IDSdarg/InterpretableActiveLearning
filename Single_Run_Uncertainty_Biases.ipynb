{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Processing Propublica Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv\n",
    "df = pd.read_csv('recidivism-2-years.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recommended cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See cell 2 here https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb\n",
    "df = df[(df[\"is_recid\"] != -1) & (df[\"days_b_screening_arrest\"] <= 30) & (df[\"days_b_screening_arrest\"] >= -30) & (df[\"c_charge_degree\"] != \"O\") & (df[\"score_text\"] != \"N/A\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "did_recidivate = df['two_year_recid'].as_matrix()\n",
    "dec_score = df['decile_score'].as_matrix()\n",
    "racial_desc = df[\"race\"]\n",
    "\n",
    "# Retaining only those attributes allowed to be used in recidivism predictions and sentencing\n",
    "columns_to_keep = [\"sex\", \"age\", \"age_cat\", \"juv_fel_count\", \"juv_misd_count\",\n",
    "                   \"juv_other_count\", \"priors_count\", \"c_charge_degree\"]\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get categorical features and dump redundant\n",
    "df_bin = pd.get_dummies(df)\n",
    "df_bin = df_bin.drop('sex_Female', axis=1)\n",
    "df_bin = df_bin.drop('c_charge_degree_M', axis=1)\n",
    "\n",
    "X = df_bin.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All equivalent in this binary case\n",
    "def uncertainty_sample_proba(clf, x):\n",
    "    probs = clf.predict_proba(x)\n",
    "    return np.amax(probs, axis=1)\n",
    "\n",
    "def top_2_dif(x):\n",
    "    x = sorted(list(x))\n",
    "    return x[-1] - x[-2]\n",
    "\n",
    "def smallest_margin_proba(clf,x):\n",
    "    probs = clf.predict_proba(x)\n",
    "    probs = np.array(map(top_2_dif, list(probs)))\n",
    "    return probs # x_pool[np.argsort(probs)[0:n]]\n",
    "    \n",
    "def label_entropy_proba(clf, x):\n",
    "    probs = clf.predict_proba(x)\n",
    "    probs = np.array(map(lambda row: sum(map(lambda y: -y*np.log2(y), row))))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pool data\n",
    "count = 0\n",
    "from random import shuffle\n",
    "idx = range(len(X))\n",
    "shuffle(idx)\n",
    "init_pool = idx[0:400]\n",
    "valida_pool = idx[-2000:]\n",
    "idx = idx[0:-2000]\n",
    "outside_pool = set(idx).difference(init_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def di_calc(clf, X, protected_attribute_indices, uncertainty_cutoff=None):\n",
    "    \"\"\"Calculate disparate impact given all minority class indices\"\"\"\n",
    "    probs = uncertainty_sample_proba(clf, X)\n",
    "    if uncertainty_cutoff is None:\n",
    "        uncertainty_cutoff = np.median(probs)\n",
    "    protected_majority = set(range(X.shape[0])) - set(protected_attribute_indices)\n",
    "    protected_majority = list(protected_majority)\n",
    "    certain = probs >= uncertainty_cutoff\n",
    "    \n",
    "    # As defined here: https://arxiv.org/pdf/1412.3756.pdf - check page 5.\n",
    "    a = float(np.count_nonzero(certain[protected_attribute_indices] == False))\n",
    "    c = float(np.count_nonzero(certain[protected_attribute_indices]))\n",
    "    b = float(np.count_nonzero(certain[protected_majority] == False))\n",
    "    d = float(np.count_nonzero(certain[protected_majority]))\n",
    "    \n",
    "    if a == 0:\n",
    "        a = 0.00001\n",
    "    if b == 0:\n",
    "        b = 0.00001\n",
    "    if c == 0:\n",
    "        c = 0.00001\n",
    "    if d == 0:\n",
    "        d = 0.00001\n",
    "    \n",
    "    pos_likelihood_ratio = (d / (b + d)) / (c / (c + a))\n",
    "\n",
    "    return 1 - 1/pos_likelihood_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note protected attributes\n",
    "race_Hispanic_ids = [index for index, is_class in enumerate((racial_desc == 'Hispanic').as_matrix()) if is_class]\n",
    "race_Asian_ids =  [index for index, is_class in enumerate((racial_desc == 'Asian').as_matrix()) if is_class]\n",
    "race_African_American =  [index for index, is_class in enumerate((racial_desc == 'African-American').as_matrix()) if is_class]\n",
    "race_Caucasian = [index for index, is_class in enumerate((racial_desc == 'Caucasian').as_matrix()) if is_class]\n",
    "\n",
    "male_ids = [index for index, is_class in enumerate((df['sex'] == 'Male').as_matrix()) if is_class]\n",
    "female_ids = [index for index, is_class in enumerate((df['sex'] == 'Female').as_matrix()) if is_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training run\n",
    "startTime = datetime.now()\n",
    "\n",
    "performances = []\n",
    "perf_ns = []\n",
    "\n",
    "clf = LogisticRegression(solver='sag', max_iter=1000, random_state=42,\n",
    "                             multi_class='multinomial')\n",
    "clf.fit(X[init_pool, :], did_recidivate[init_pool])\n",
    "predictions = clf.predict(X[valida_pool, :])\n",
    "performances.append(accuracy_score(predictions, did_recidivate[valida_pool]))\n",
    "perf_ns.append(len(init_pool))\n",
    "\n",
    "di_black = []\n",
    "di_hispanic = []\n",
    "di_asian = []\n",
    "di_white = []\n",
    "\n",
    "di_male = []\n",
    "di_female = []\n",
    "\n",
    "mean_certainties_black = []\n",
    "mean_certainties_hispanic = []\n",
    "mean_certainties_asian = []\n",
    "mean_certainties_white = []\n",
    "\n",
    "for i in range(2000):\n",
    "    probs = uncertainty_sample_proba(clf, X[sorted(outside_pool), :])\n",
    "    index = sorted(outside_pool)[np.argmin(probs)]\n",
    "    init_pool.append(index)\n",
    "    outside_pool.remove(index)\n",
    "        \n",
    "    clf.fit(X[init_pool, :], did_recidivate[init_pool])\n",
    "    predictions = clf.predict(X[valida_pool, :])\n",
    "    performances.append(accuracy_score(predictions, did_recidivate[valida_pool]))\n",
    "    perf_ns.append(len(init_pool))\n",
    "    \n",
    "    # Record disparate impact of uncertainty\n",
    "    di_black.append(di_calc(clf, X, race_African_American))\n",
    "    di_hispanic.append(di_calc(clf, X, race_Hispanic_ids))\n",
    "    di_asian.append(di_calc(clf, X, race_Asian_ids))\n",
    "    di_white.append(di_calc(clf, X, race_Caucasian))\n",
    "    \n",
    "    di_male.append(di_calc(clf, X, female_ids))\n",
    "    di_male.append(di_calc(clf, X, male_ids))\n",
    "    \n",
    "    # Record mean certainties\n",
    "    probs = uncertainty_sample_proba(clf, X)\n",
    "    mean_certainties_black.append(np.mean(probs[race_African_American]))\n",
    "    mean_certainties_hispanic.append(np.mean(probs[race_Hispanic_ids]))\n",
    "    mean_certainties_asian.append(np.mean(probs[race_Asian_ids]))\n",
    "    mean_certainties_white.append(np.mean(probs[race_Caucasian]))\n",
    "\n",
    "    assert(len(performances)==len(perf_ns))\n",
    "\n",
    "print datetime.now() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_num = 0\n",
    "with open('all_performances_{}.pl'.format(run_num), 'wb') as f:\n",
    "    pickle.dump([di_black,di_hispanic, di_asian, di_white, di_male, di_female, mean_certainties_black, mean_certainties_hispanic, mean_certainties_asian, mean_certainties_white, performances], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9), dpi=300)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_black)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_hispanic)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_asian)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_white)\n",
    "plt.xlabel('Labeled Pool Size')\n",
    "plt.ylabel('Mean Certainty')\n",
    "plt.legend(['Black', 'Hispanic', 'Asian', 'White'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9), dpi=300)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_black)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_hispanic)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_asian)\n",
    "plt.plot(perf_ns[:-1], mean_certainties_white)\n",
    "plt.xlabel('Labeled Pool Size')\n",
    "plt.ylabel('Mean Certainty')\n",
    "plt.legend(['Black', 'Hispanic', 'Asian', 'White'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9), dpi=300)\n",
    "plt.plot(perf_ns[1:], di_black)\n",
    "plt.plot(perf_ns[1:], di_hispanic)\n",
    "plt.plot(perf_ns[1:], di_asian)\n",
    "plt.plot(perf_ns[1:], di_white)\n",
    "plt.legend(['Black', 'Hispanic', 'Asian', 'White'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "plt.figure(figsize=(16,9), dpi=200)\n",
    "plt.plot(perf_ns[1:], di_black)\n",
    "plt.plot(perf_ns[1:], di_hispanic)\n",
    "plt.plot(perf_ns[1:], di_white)\n",
    "plt.legend(['Black', 'Hispanic', 'White'], loc='best')\n",
    "# plt.savefig('LongerTrialUncertaintyBias_WIDE2.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8), dpi=200)\n",
    "plt.plot(perf_ns[1:], di_black, color=plt.cm.Paired(0.1))\n",
    "plt.plot(perf_ns[1:], di_hispanic, color=plt.cm.Paired(0.2))\n",
    "plt.plot(perf_ns[1:], di_white, color=plt.cm.Paired(0.4))\n",
    "# plt.legend(['Black', 'Hispanic', 'White'], loc='best')\n",
    "plt.xlabel('Labeled Pool Size')\n",
    "plt.ylabel('Uncertainty Bias')\n",
    "# plt.savefig('FINAL_GRAPHX/Black_Hispanic_White_Plain_AL_Comparison_Final_FULL_WIDE.eps', format='eps')\n",
    "# plt.legend(['UNCERTAINTY BIAS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8), dpi=200)\n",
    "fig, ax1 = plt.subplots(figsize=(8,8), dpi=200)\n",
    "plt.plot(perf_ns[:-1], di_black, color=plt.cm.Paired(0.1))\n",
    "plt.plot(perf_ns[:-1], di_hispanic, color=plt.cm.Paired(0.2))\n",
    "plt.plot(perf_ns[:-1], di_white, color=plt.cm.Paired(0.4))\n",
    "plt.plot(perf_ns, performances, color=(0, 0, 0))\n",
    "# plt.legend(['Black', 'Hispanic', 'White'], loc='best')\n",
    "plt.xlabel('Labeled Pool Size')\n",
    "plt.ylabel('Uncertainty Bias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "choice_counts = [i[0] for i in choice_counts]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "vectors = []\n",
    "for choice in choice_counts:\n",
    "    vectors.append([0 if index != choice else 1 for index in range(len(set(choice_counts)))])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.cm.Paired(i)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(np.array(vectors).shape[1]):\n",
    "    plt.plot(perf_ns, np.cumsum(vectors, axis=0)[:, i], color=plt.cm.Paired(float(i)/clust_number))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.cm.Paired(1.0)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# center_labels = []\n",
    "# for center in cluster.cluster_centers_:\n",
    "#     center_labels.append(np.argsort(np.round(np.absolute(center), decimals=2))[-2:])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "init_pool\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "race_cumulative_sums[400:, i].shape\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.figure(figsize=(8,8), dpi=400)\n",
    "\n",
    "race_cumulative_sums = []\n",
    "for i in init_pool:\n",
    "    if i in race_Hispanic_ids:\n",
    "        race_cumulative_sums.append(np.array([1,0,0,0]))\n",
    "    elif i in race_Asian_ids:\n",
    "        race_cumulative_sums.append(np.array([0,1,0,0]))\n",
    "    elif i in race_African_American:\n",
    "        race_cumulative_sums.append(np.array([0,0,1,0]))\n",
    "    elif i in race_Caucasian:\n",
    "        race_cumulative_sums.append(np.array([0,0,0,1]))\n",
    "    else:\n",
    "        race_cumulative_sums.append(np.array([0,0,0,0]))\n",
    "    \n",
    "race_cumulative_sums = np.cumsum(race_cumulative_sums, axis=0)\n",
    "\n",
    "for i in range(4):\n",
    "     plt.plot(perf_ns[:-1], race_cumulative_sums[400:, i], color=plt.cm.Paired(float(i)/4), linewidth=3)\n",
    "\n",
    "plt.xlabel(\"Total training set size\")\n",
    "plt.ylabel(\"Number of samples queried from pool\")\n",
    "plt.legend(['Hispanic', 'Asian', 'Black', 'White'], loc='best')\n",
    "plt.savefig('Race_query_countsBT.eps', format='eps')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.figure(figsize=(8,8), dpi=400)\n",
    "\n",
    "race_cumulative_sums = []\n",
    "for i in init_pool:\n",
    "    if i in race_Hispanic_ids:\n",
    "        race_cumulative_sums.append(np.array([1,0,0,0]))\n",
    "    elif i in race_African_American:\n",
    "        race_cumulative_sums.append(np.array([0,0,1,0]))\n",
    "    elif i in race_Caucasian:\n",
    "        race_cumulative_sums.append(np.array([0,0,0,1]))\n",
    "    else:\n",
    "        race_cumulative_sums.append(np.array([0,0,0,0]))\n",
    "    \n",
    "race_cumulative_sums = np.cumsum(race_cumulative_sums, axis=0)\n",
    "\n",
    "colors = [0.2, 0.1, 0.4]\n",
    "for i, total_head_count, color in zip([0, 2, 3], [637, 3696, 2454], colors):\n",
    "     plt.plot(perf_ns[:-1], race_cumulative_sums[400:, i]/float(total_head_count), color=plt.cm.Paired(color), linewidth=2.5)\n",
    "\n",
    "plt.xlabel(\"Total training set size\")\n",
    "plt.ylabel(\"Proportion of total population in dataset\")\n",
    "plt.legend(['Hispanic', 'Black', 'White'], loc='best')\n",
    "plt.savefig('Race_query_proportionBT.eps', format='eps')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "population_counts = [637,32,3696,2454]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.plot(race_cumulative)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "race_Hispanic_ids = [index for index, is_class in enumerate((df['race'] == 'Hispanic').as_matrix()) if is_class]\n",
    "race_Asian_ids =  [index for index, is_class in enumerate((df['race'] == 'Asian').as_matrix()) if is_class]\n",
    "race_African_American =  [index for index, is_class in enumerate((df['race'] == 'African-American').as_matrix()) if is_class]\n",
    "race_Caucasian = [index for index, is_class in enumerate((df['race'] == 'Caucasian').as_matrix()) if is_class]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# legend = [list(df_exp.columns)[i] + '        &         ' + list(df_exp.columns)[v] for i,v  in center_labels]\n",
    "legend = []\n",
    "\n",
    "for i in center_labels:\n",
    "    label = ''\n",
    "    \n",
    "    for v in i:\n",
    "        label += list(df_exp.columns)[v]\n",
    "        if v != i[-1]:\n",
    "            label += '   &     '\n",
    "        else:\n",
    "            pass\n",
    "    if label == '':\n",
    "        label = 'Many sources'\n",
    "    legend.append(label)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "legend\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.figure(figsize=(12,12), dpi=5)\n",
    "\n",
    "\n",
    "for i in range(np.array(vectors).shape[1]):\n",
    "    plt.plot(perf_ns, np.cumsum(vectors, axis=0)[:, i], color=plt.cm.Paired(float(i)/clust_number))\n",
    "\n",
    "# plt.set_linewidth(44)\n",
    "plt.xlabel(\"Total training set size\")\n",
    "plt.ylabel(\"Number of samples from a cluster\",)\n",
    "plt.legend(legend, loc='best')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Highly explored curves\n",
    "high_indices = []\n",
    "high_legend = []\n",
    "\n",
    "for index, (label, count) in enumerate(zip(legend, np.cumsum(vectors, axis=0)[-1])):\n",
    "    if count >= 90:\n",
    "        high_indices.append((index, count, label))\n",
    "        high_legend.append(label)\n",
    "        \n",
    "high_indices, counts, high_legend = zip(*sorted(high_indices, key=lambda x:x[1], reverse=True))\n",
    "        \n",
    "        \n",
    "        \n",
    "plt.figure(figsize=(8,8))    \n",
    "    \n",
    "for i in high_indices:\n",
    "        plt.plot(perf_ns, np.cumsum(vectors, axis=0)[:, i], color=plt.cm.Paired(float(i)/clust_number), linewidth=2.5)\n",
    "\n",
    "# plt.set_linewidth(44)\n",
    "plt.xlabel(\"Total training set size\")\n",
    "plt.ylabel(\"n\")\n",
    "plt.legend(high_legend, loc='best')\n",
    "plt.savefig(\"HighFreqCertaintyCounts.eps\", format='eps')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Highly explored curves\n",
    "# high_indices = []\n",
    "# high_legend = []\n",
    "# for index, (label, count) in enumerate(zip(legend, np.cumsum(vectors, axis=0)[-1])):\n",
    "#     if count >= 100:\n",
    "#         high_indices.append(index)\n",
    "#         high_legend.append(label)\n",
    "        \n",
    "plt.figure(figsize=(8,8))    \n",
    "    \n",
    "for i in high_indices:\n",
    "        plt.plot(perf_ns, np.array(cluster_dis)[:, i], color=plt.cm.Paired(float(i)/clust_number))\n",
    "\n",
    "# plt.set_linewidth(44)\n",
    "plt.xlabel(\"Total training set size\")\n",
    "plt.ylabel(\"Uncertainty Bias\")\n",
    "# plt.legend(high_legend, loc='best')\n",
    "\n",
    "plt.savefig(\"HighFreqCertaintyBias_NO_LEGEND.eps\", format='eps')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "fontP = FontProperties()\n",
    "fontP.set_size('medium')\n",
    "\n",
    "# Highly underexplored curves\n",
    "high_indices = []\n",
    "high_legend = []\n",
    "for index, (label, count) in enumerate(zip(legend, np.cumsum(vectors, axis=0)[-1])):\n",
    "    if count <= 20:\n",
    "        high_indices.append(index)\n",
    "        high_legend.append(label)\n",
    "        \n",
    "plt.figure(figsize=(10, 20))    \n",
    "for i in range(np.array(vectors).shape[1]):\n",
    "    if i in high_indices:\n",
    "        plt.plot(perf_ns, np.cumsum(vectors, axis=0)[:, i], color=plt.cm.Paired(float(i)/clust_number))\n",
    "\n",
    "# plt.set_linewidth(44)\n",
    "plt.xlabel(\"Total training set size\")\n",
    "plt.ylabel(\"n\")\n",
    "plt.legend(high_legend, loc='best', prop = fontP)\n",
    "plt.savefig(\"LowFreqCertaintyCounts.eps\", format='eps')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Highly explored curves\n",
    "high_indices = []\n",
    "high_legend = []\n",
    "for index, (label, count) in enumerate(zip(legend, np.cumsum(vectors, axis=0)[-1])):\n",
    "    if count <= 20:\n",
    "        high_indices.append(index)\n",
    "        high_legend.append(label)\n",
    "        \n",
    "plt.figure(figsize=(8,8))    \n",
    "    \n",
    "for i in range(np.array(vectors).shape[1]):\n",
    "    if i in high_indices:\n",
    "        plt.plot(perf_ns, np.array(cluster_dis)[:, i], color=plt.cm.Paired(float(i)/clust_number))\n",
    "\n",
    "# plt.set_linewidth(44)\n",
    "plt.xlabel(\"Total training set size\")\n",
    "plt.ylabel(\"Uncertainty Bias\")\n",
    "plt.legend(high_legend, loc='best')\n",
    "\n",
    "plt.savefig(\"LowFreqCertaintyBias_TALL.eps\", format='eps')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Highly explored curves\n",
    "high_indices = []\n",
    "high_legend = []\n",
    "for index, (label, count) in enumerate(zip(legend, np.cumsum(vectors, axis=0)[-1])):\n",
    "    if count <= 20:\n",
    "        high_indices.append(index)\n",
    "        high_legend.append(label)\n",
    "        \n",
    "plt.figure(figsize=(30,40))    \n",
    "    \n",
    "for i in range(np.array(vectors).shape[1]):\n",
    "    if i in high_indices:\n",
    "        plt.plot(perf_ns, np.cumsum(vectors, axis=0)[:, i], color=plt.cm.Paired(float(i)/clust_number))\n",
    "\n",
    "# plt.set_linewidth(44)\n",
    "plt.xlabel(\"Total training set size\", fontproperties=font_prop)\n",
    "plt.ylabel(\"n\", fontproperties=font_prop)\n",
    "plt.legend(high_legend, prop=font_prop, loc='upper left')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "plt.plot(perf_ns, performances)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "di_calc(clf, X, female_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
